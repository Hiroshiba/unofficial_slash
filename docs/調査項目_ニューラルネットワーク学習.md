# 調査項目: ニューラルネットワーク・学習関連

## 1. NANSY++ Pitch Encoder アーキテクチャ詳細

### 質問内容
SLASH の Pitch Encoder が基づく NANSY++ の具体的なネットワーク構造（層数、チャンネル数、活性化関数）の詳細は？

### 背景
- 論文 2.2節: "The architecture of the Pitch Encoder is based on the Pitch encoder of [12]"
- 入力: CQT (T×176) → 出力: F0確率分布 P (T×1024) + BAP B (T×8)
- NANSY++からの改変点（確率分布出力、BAP追加）が不明

### 調査ポイント
1. NANSY++ Pitch Encoder の詳細構造
   - Convolution層の設計（カーネルサイズ、ストライド、チャンネル数）
   - Normalization の種類（BatchNorm/LayerNorm）
   - 活性化関数の選択
2. SLASH での改変内容
   - 確率分布出力への変更方法
   - BAP出力の追加方法
3. 時系列長の保持方法
4. 計算量とメモリ使用量

### 参考資料
- Choi et al. 2022: "NANSY++: Unified voice synthesis with neural analysis and synthesis"
- docs/参考文献1.md の詳細記述

---

## 2. 対数周波数スケールマッピング

### 質問内容
F0確率分布 P の対数周波数スケール（20Hz-2kHz、1024bins）への正確なマッピング式は？

### 背景
- 論文: "1024-dimensional F0 probability distribution mapped onto a logarithmic frequency scale ranging from 20 Hz to 2 kHz"
- 重み付き平均で F0 値 p を計算するため、正確な周波数-bin対応が必要

### 調査ポイント
1. 対数スケールの底（log2? log10? ln?）
2. 20Hz-2kHz の1024bins分割式
3. 重み付き平均計算での周波数値抽出
4. Nyquist周波数 (12kHz) との関係
5. CQT bins (205) から F0 bins (1024) への変換

### 参考資料
- 一般的な対数周波数スケール実装
- 音楽情報検索でのピッチ表現手法

---

## 3. Huber Loss のデルタパラメータ

### 質問内容
Pitch Consistency Loss (Equation 1) で使用される Huber loss h(·) のデルタパラメータの適切な値は？

### 背景
- 論文では h(·) denotes the Huber norm のみ記載
- デルタ値により L2 (小誤差) と L1 (大誤差) の境界が決まる
- ピッチ推定精度に直接影響

### 調査ポイント
1. 音声ピッチ推定でのHuber loss 標準設定値
2. SPICE論文での設定値
3. 他のSSL手法での使用例
4. デルタ値の感度分析結果
5. オクターブ誤差に対する頑健性

### 参考資料
- SPICE論文: "SPICE: Self-supervised pitch estimation"  
- Huber loss の理論と応用例
- 音響信号処理でのロバスト推定手法

---

## 4. Dynamic Batching の実装詳細

### 質問内容
論文で言及される「dynamic batching with an average batch size of 17 samples」の具体的な実装方法は？

### 背景
- 論文 3.1節: "we utilized a dynamic batching with an average batch size of 17 samples to create mini-batches"
- ESPnet-TTS[27]が参考文献として挙げられている
- 可変長音声データの効率的処理が目的

### 調査ポイント
1. Dynamic batching の基本原理
2. バッチサイズ決定アルゴリズム
   - 音声長による調整方法
   - メモリ使用量の制約
   - GPU利用効率の最適化
3. ESPnet-TTS での実装例
4. パディング処理との併用
5. 学習安定性への影響

### 参考資料
- Hayashi et al. 2020: "ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit"
- PyTorch DataLoader の可変長バッチ処理
- 音声深層学習でのバッチ処理最適化手法

---

## 5. 学習最適化の詳細設定

### 質問内容
AdamW オプティマイザ（lr=0.0002）以外の学習設定（スケジューラ、正則化、早期停止など）は？

### 背景
- 論文では学習率のみ明記
- 100,000 steps の長期学習での安定化手法が必要
- 複数損失の重み付き学習における最適化

### 調査ポイント
1. 学習率スケジューラの選択
   - Cosine annealing
   - Step decay
   - Warm-up の必要性
2. 正則化手法
   - Weight decay の値
   - Gradient clipping
   - Dropout の使用
3. 早期停止の基準
4. チェックポイント保存間隔
5. 検証データでの評価頻度

### 参考資料
- AdamW論文: "Decoupled weight decay regularization"
- 音声深層学習での最適化手法
- SSL学習での安定化技法