以下に、ご指定いただいた低優先（実装後の最適化で調査）の項目についての調査結果を記載します。

---

### 🟢 低優先（実装後の最適化で調査）の項目

#### 1. Filter Scaleの効果と実装

**質問:**
CQTパラメータの `filter_scale=0.5` 設定の具体的な効果と、`librosa`/`torchaudio`での正確な実装方法は何か？

**調査結果:**
`filter_scale`パラメータは、CQT（Constant-Q Transform）で使用される各フィルタの帯域幅を調整する役割を持ちます。この値は時間分解能と周波数分解能のトレードオフを制御します。

*   **効果**:
    *   **標準設定 (`filter_scale=1.0`)**: 各フィルタの帯域幅は、その中心周波数に比例して設定されます。これはCQTの基本的な定義です。
    *   **`filter_scale < 1.0` の場合 (例: 0.5)**: フィルタの帯域幅が標準よりも狭くなります。これにより、**周波数分解能が向上**する一方で、時間分解能は低下します。しかし、論文で「急速なピッチ変化に対応するため」と述べられているのは直感に反するように見えます。より詳細な調査によると、`librosa`の実装では`filter_scale`が小さいほど時間領域でのフィルタ長が短くなり、**時間分解能が向上**します。これにより、急なピッチの変化に対する追従性が高まると考えられます。
    *   **`filter_scale > 1.0` の場合**: フィルタの帯域幅が広がり、時間分解能が低下する代わりに周波数分解能が向上します。

*   **実装方針**:
    *   **`librosa.cqt`**: `tuning`パラメータ、またはより直接的には非公開APIの`librosa.filters.constant_q`でフィルタを自作することで調整可能です。しかし、最も簡単な方法は、`librosa.cqt`の`filter_scale`引数を直接使用することです。
        ```python
        import librosa
        # librosa.cqtのfilter_scale引数を使用
        cqt_spectrogram = librosa.cqt(y=audio_signal, sr=sample_rate,
                                      hop_length=120, # 5ms at 24kHz
                                      fmin=32.70,
                                      n_bins=205,
                                      bins_per_octave=24,
                                      filter_scale=0.5)
        ```
    *   **`torchaudio.transforms.CQT`**: こちらの実装には`filter_scale`に直接対応するパラメータは存在しません。同様の効果を得るには、カスタムフィルタバンクを設計し、`resample_method`などのパラメータを調整する必要がありますが、複雑になります。

*   **結論**: `librosa`の実装を用いるのが最も簡単かつ直接的です。`filter_scale=0.5`は、時間分解能を高め、論文の主張通り急なピッチ変動への追従性を向上させるためのチューニングと考えられます。

---

#### 2. Huber Lossのパラメータ

**質問:**
Pitch Consistency Loss (Equation 1) で使用されるHuber Loss `h(·)`のデルタ（δ）パラメータの適切な値は何か？

**調査結果:**
SLASH論文ではHuber Lossのデルタパラメータ（または閾値τ）について言及がありません。しかし、類似の自己教師あり学習手法である**SPICE論文**で具体的な設定が記載されています。

*   **SPICE論文での設定**:
    SPICEでは、Huber Lossの閾値`τ`を、ピッチ差のスケーリング係数`σ`に比例させて`τ = 0.25 * σ`と設定しています。 [SPICE, p. 5] `σ`は、エンコーダ出力（0〜1の範囲）と実際のピッチ差（CQTビン単位）を対応付けるための係数です。 [SPICE, p. 5]

    この設定は、誤差がCQTのビン分解能やピッチのダイナミックレンジに応じて適応的に決まるため、非常に合理的です。

*   **実装方針**:
    SPICE論文のアプローチを踏襲し、Huber Lossのデルタを適応的に設定することを推奨します。

    1.  **スケーリング係数`σ`の計算**: SPICE論文の式(9)に基づき、SLASHのパラメータで`σ`を計算します。
        *   `Q = 24` (bins_per_octave)
        *   `f_max = 2000` Hz
        *   `f_min = 20` Hz
        *   `σ = 1 / (Q * log2(f_max / f_min))`
           `σ = 1 / (24 * log2(100)) ≈ 1 / (24 * 6.64) ≈ 0.00627`
    2.  **デルタ`δ`（τ）の設定**:
        *   `delta = 0.25 * σ ≈ 0.00157`
    3.  **PyTorchでの実装**:
        ```python
        import torch.nn as nn
        # deltaは上記で計算した値
        huber_loss_fn = nn.HuberLoss(delta=0.00157)
        loss = huber_loss_fn(predicted_pitch_diff, target_pitch_diff)
        ```

*   **結論**: 固定値ではなく、SPICE論文を参考にCQTのパラメータから導出される適応的なデルタ値（`δ ≈ 0.00157`）を使用することが、モデルの頑健性を高める上で最適と考えられます。

---

#### 3. 学習最適化設定

**質問:**
AdamWオプティマイザ（lr=0.0002）以外の学習設定（スケジューラ、正則化、早期停止など）は何か？

**調査結果:**
SLASH論文では学習率以外の詳細な最適化設定は省略されています。しかし、関連する最新の音声生成・分析モデルの論文から、一般的に採用されるベストプラクティスを推測できます。

*   **関連論文での設定**:
    *   **DDSP論文**: 学習率`0.001`のADAMオプティマイザを使用し、10,000ステップごとに`0.98`を乗じる**指数関数的学習率減衰（Exponential Decay）**を採用しています。 [DDSP, p. 16]
    *   **A Spectral Energy Distance for Parallel Speech Synthesis論文**: **線形ウォームアップ（Linear Warmup）**を6,000ステップにわたって行う学習率スケジュールを使用しています。 [A Spectral Energy Distance for Parallel Speech Synthesis, p. 14]
    *   **NANSY++論文**: ほとんどのモジュールで固定学習率のAdamまたはAdamWを使用しており、スケジューラの言及はありません。 [NANSY++, p. 4, 18, 8]

*   **実装方針**:
    100,000ステップという長期の学習を安定させるためには、学習率スケジューラの導入が推奨されます。

    1.  **学習率スケジューラ**:
        *   **推奨案1（シンプル）**: DDSPで採用されている**指数関数的減衰**。実装が容易で安定した学習が期待できます。
        *   **推奨案2（一般的）**: **線形ウォームアップとコサイン減衰（Cosine Annealing with Warmup）**の組み合わせ。近年の大規模モデルで広く採用されており、学習初期の安定化と終盤の精密な調整に効果的です。ウォームアップは5,000〜10,000ステップ程度が一般的です。
    2.  **正則化**:
        *   論文で`AdamW`が指定されていることから、**Decoupled Weight Decay**の使用が前提となります。`weight_decay`の値は`1e-2`などが一般的ですが、モデルサイズに応じて調整が必要です。
        *   **Gradient Clipping**: 勾配爆発を防ぐため、勾配のノルムを`1.0`などでクリッピングすることが有効です。
    3.  **早期停止（Early Stopping）**:
        *   必須ではありませんが、長時間の学習における過学習を防ぐため、検証用データセットでの損失（特に`log-F0 RMSE`や`RPA`）を監視し、数エポック（例: 5〜10エポック）改善が見られない場合に学習を停止する設定が考えられます。

*   **結論**: 論文に記載はありませんが、**ウォームアップ付きの学習率スケジューラ**（コサイン減衰など）と、`AdamW`による**Weight Decay**、そして**Gradient Clipping**を導入することが、10万ステップの学習を成功させるための標準的な実践となります。これらのパラメータは、最終的な性能を微調整する上で重要な要素です。