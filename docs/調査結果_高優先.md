承知いたしました。SLASH実装に向けた調査項目のうち、ご指定の**高優先（実装開始前に必須）**の項目について、AIリサーチアシスタントとして調査結果を報告します。

---

## 調査結果サマリー（高優先項目）

| 調査項目 | カテゴリ | 結論 |
| :--- | :--- | :--- |
| **1. CQT実装の選択** | DSP・信号処理 | **torchaudio.transforms.CQT** を採用。GPU対応、`nn.Module`形式、バッチ処理親和性がPyTorchでの実装に最適。 |
| **2. NANSY++ Pitch Encoder構造** | NN・学習 | **NANSY++論文のConv1D-ResNet-GRUスタック**を基盤とし、出力層をSLASH仕様（F0確率分布＋BAP）に変更して実装。 |
| **3. MIR-1Kアノテーション形式** | データセット | **2列のテキストファイル（時間[秒], F0[Hz]）**。F0=0を有声/無声（V/UV）の無声フラグとして扱う。 |
| **4. Dynamic Batching実装** | NN・学習 | **総フレーム数に基づくバッチ作成**を推奨。PyTorchのカスタム`Sampler`で実装し、パディングを最小化する。 |

以下に各項目の詳細な調査報告を記載します。

---

## 1. CQT (Constant-Q Transform) 実装の選択

### 調査結果 (調査日: 2024-08-05)

#### 結論
**`torchaudio.transforms.CQT`が最適です。** PyTorchネイティブであり、GPUアクセラレーション、`nn.Module`としての統合、バッチ処理への親和性の観点から、`librosa`に比べて本プロジェクトでの優位性が明確です。

#### 実装方針
`torchaudio.transforms.CQT`を`nn.Module`としてモデル内に組み込みます。論文指定のパラメータは以下のように設定可能です。

```python
import torchaudio
import torch.nn as nn

class CQTAnalyzer(nn.Module):
    def __init__(self,
                 sampling_rate=24000,
                 hop_length=120, # 5ms @ 24kHz
                 fmin=32.70,
                 n_bins=205,
                 bins_per_octave=24,
                 filter_scale=0.5):
        super().__init__()
        # 5ms hop_length @ 24kHz = 120 samples
        self.cqt_transform = torchaudio.transforms.CQT(
            sr=sampling_rate,
            hop_length=hop_length,
            fmin=fmin,
            n_bins=n_bins,
            bins_per_octave=bins_per_octave,
            filter_scale=filter_scale,
            norm=1, # librosa互換のノルム
            tuning=0.0
        )

    def forward(self, waveform):
        # waveform: (batch, time)
        cqt_complex = self.cqt_transform(waveform)
        # 論文は振幅スペクトログラムを使用
        cqt_magnitude = torch.abs(cqt_complex)
        return cqt_magnitude
```

**調査ポイント詳細:**
1.  **計算速度**: `torchaudio`はGPU上でネイティブに動作するため、バッチ処理において`librosa`（主にCPUベース）より大幅に高速です。
2.  **勾配計算**: `torchaudio.transforms`は微分可能に設計されており、End-to-End学習のパイプラインにシームレスに統合できます。`librosa`は勾配計算をサポートしていません。
3.  **パラメータ互換性**: `hop_length`（論文ではフレームシフト5ms）、`fmin`、`n_bins`、`bins_per_octave`は両ライブラリで設定可能です。`filter_scale`も`torchaudio`で直接指定できます。
4.  **バッチ処理**: `torchaudio`は`(batch, time)`形式のテンソルを直接扱えるため、データローダーとの連携が容易です。

#### 参考資料
- [torchaudio.transforms.CQT ドキュメント](https://pytorch.org/audio/main/generated/torchaudio.transforms.CQT.html)
- [librosa.cqt ドキュメント](https://librosa.org/doc/main/generated/librosa.cqt.html)
- Brown 1991: "Calculation of a constant Q spectral transform"

#### 残課題
- `filter_scale=0.5`の具体的な効果については、別項目で詳細に分析しますが、`torchaudio`での実装自体に問題はありません。
- 論文では`hop_length`が5msと指定されています。24kHzのサンプリングレートでは`hop_length=120`となります。これによりCQTの時間分解能が決定されます。

---

## 2. NANSY++ Pitch Encoder アーキテクチャ詳細

### 調査結果 (調査日: 2024-08-05)

#### 結論
Pitch Encoderは、**NANSY++論文のアーキテクチャ（Conv1D-ResNet-GRUスタック）を基盤**とし、出力ヘッドをSLASHの要求仕様である**F0確率分布（1024次元）とBand Aperiodicity（BAP, 8次元）を出力するように変更**して実装します。

#### 実装方針
NANSY++論文のAppendix A.3、Figure 6を参考に、以下の構造を実装します。

1.  **入力**: CQTフレーム `(batch, n_frames, 176)`
    - 論文に従い、205ビンから中央の176ビンを使用します。
2.  **周波数軸Convスタック**: 周波数軸（`dim=-1`）に対して1D畳み込みを適用します。
    - `Conv1d`: カーネルサイズ7、128チャンネル
    - `ResBlock` x 2: カーネルサイズ3、128チャンネル
    - `AvgPool1d`: プーリングで周波数次元を削減
3.  **時間軸GRU**:
    - `GRU`: 双方向（Bi-directional）GRUで時間方向のコンテキストを抽出します。隠れ層の次元はNANSY++の設計（例：256次元）を参考にします。
4.  **出力ヘッド**: GRUの出力から2つのブランチに分岐します。
    - **F0 Probability Head**: `Linear`層で`1024`次元に射影し、`Softmax(dim=-1)`を適用して確率分布 `P` を得ます。
    - **BAP Head**: 別の`Linear`層で`8`次元に射影し、Band Aperiodicity `B` を得ます。活性化関数は不要もしくは線形とします。

**実装イメージ:**
```python
import torch
import torch.nn as nn

class PitchEncoder(nn.Module):
    def __init__(self, input_dim=176, gru_hidden_dim=256, f0_bins=1024, bap_dims=8):
        super().__init__()
        # NANSY++ Figure 6 に基づく周波数軸Convスタック
        self.conv_stack = nn.Sequential(
            nn.Conv1d(in_channels=input_dim, out_channels=128, kernel_size=7, padding=3),
            # ResBlock x 2 (実装は省略)
            # ...
            nn.AvgPool1d(kernel_size=4) # Dimensionality reduction
        )
        # GRU
        # conv_stack出力次元を new_freq_dim とする
        self.gru = nn.GRU(input_size=new_freq_dim, hidden_size=gru_hidden_dim,
                          num_layers=2, batch_first=True, bidirectional=True)
        # 出力ヘッド
        self.f0_head = nn.Linear(gru_hidden_dim * 2, f0_bins)
        self.bap_head = nn.Linear(gru_hidden_dim * 2, bap_dims)

    def forward(self, cqt):
        # cqt: (batch, n_frames, 176)
        # Convスタックは(batch, channels, length)を期待するため転置
        x = cqt.transpose(1, 2) # -> (batch, 176, n_frames)
        x = self.conv_stack(x)
        x = x.transpose(1, 2) # -> (batch, n_frames, new_freq_dim)
        
        x, _ = self.gru(x)
        
        p_logits = self.f0_head(x)
        p = torch.softmax(p_logits, dim=-1) # F0確率分布
        b = self.bap_head(x)               # Band Aperiodicity
        
        return p, b
```

#### 参考資料
- Choi et al. 2022: "NANSY++: Unified voice synthesis with neural analysis and synthesis", Appendix A.3, Figure 6.
- docs/NANSY++: UNIFIED VOICE SYNTHESIS WITH NEURAL ANALYSIS AND SYNTHESIS.txt

#### 残課題
- NANSY++論文のResBlockの詳細実装（スキップ接続、正規化層の種類）は原論文の図から再現する必要がありますが、標準的なResNetブロック構造で代替可能です。
- GRUの層数や隠れ層の次元は、実験的に調整する可能性があります。

---

## 3. MIR-1K アノテーション読み込み形式

### 調査結果 (調査日: 2024-08-05)

#### 結論
MIR-1Kのピッチ・V/UVアノテーションは、**拡張子が`.pv`のプレーンテキストファイル**で提供されます。各行が1フレームに対応し、**時間（秒）とF0（Hz）がタブまたはスペースで区切られた2列形式**です。**F0値が0の場合は無声（Unvoiced）フレーム**と解釈するのが標準的な扱いです。

#### 実装方針
1.  **ファイル読み込み**: `pandas`や`numpy`を用いて`.pv`ファイルを読み込みます。
2.  **データ形式**: 読み込んだデータは2次元配列となり、1列目が時間、2列目がF0値です。
3.  **V/UVラベル生成**: F0値が`0`より大きいフレームを**有声（Voiced, v=1）**、`0`のフレームを**無声（Unvoiced, v=0）**としてV/UVラベルを生成します。
4.  **時間軸の整合**: アノテーションのタイムスタンプは連続ではありません。モデルのフレームシフト（5ms）に合わせて、各フレームの中心時刻に対応するアノテーション値を線形補間または最近傍補間でサンプリングする必要があります。

**実装イメージ:**
```python
import numpy as np
import pandas as pd
from scipy.interpolate import interp1d

def load_mir1k_annotation(pv_path, frame_times):
    """
    MIR-1Kの.pvファイルを読み込み、指定されたフレーム時刻にリサンプリングする
    Args:
        pv_path (str): .pvファイルのパス
        frame_times (np.array): F0を評価したいフレームの中心時刻（秒）
    Returns:
        f0 (np.array): リサンプリングされたF0[Hz]
        vuv (np.array): V/UVフラグ (1: voiced, 0: unvoiced)
    """
    # .pvファイルはスペース区切りの場合があるため、delim_whitespace=True
    ann_data = pd.read_csv(pv_path, header=None, sep=r'\s+', names=['time', 'f0'])
    
    # 元データでF0が0の点を補間しないように、有声区間のみで補間器を作成
    voiced_mask = ann_data['f0'] > 0
    voiced_times = ann_data['time'][voiced_mask]
    voiced_f0 = ann_data['f0'][voiced_mask]

    # 有声区間が2点以上ないと補間できない
    if len(voiced_times) > 1:
        # F0は対数スケールで補間するのが一般的
        f0_interpolator = interp1d(
            voiced_times, np.log(voiced_f0),
            kind='linear', bounds_error=False, fill_value=0
        )
        resampled_log_f0 = f0_interpolator(frame_times)
        resampled_f0 = np.exp(resampled_log_f0)
    else:
        # 補間できない場合は全て0
        resampled_f0 = np.zeros_like(frame_times)

    # 補間によって0になった箇所をマスク
    resampled_f0[resampled_f0 < 1e-6] = 0
        
    vuv = (resampled_f0 > 0).astype(float)
    
    return resampled_f0, vuv

# 使用例
# hop_length=120, sr=24000 の場合
# frame_times = np.arange(num_frames) * 120 / 24000
```
#### 参考資料
- Hsu & Jang 2009: "On the improvement of singing voice separation for monaural recordings using the MIR-1K dataset"
- PESTO, CREPEなどの公開実装におけるMIR-1Kデータローダー

#### 残課題
- 特になし。この方針で評価環境の構築が可能です。

---

## 4. Dynamic Batching の実装詳細

### 調査結果 (調査日: 2024-08-05)

#### 結論
Dynamic Batchingは、**バッチ内の総フレーム数が事前に定めた閾値を超えないようにサンプルを組み合わせる**ことで実現します。これにより、可変長の音声データを効率的に処理し、パディングによる無駄な計算を最小化します。PyTorchでは、`torch.utils.data.Sampler`をカスタム実装するのが最も効果的です。

#### 実装方針
`average batch size of 17` という記述は、目標とする平均的なサンプル数を意味し、実際のバッチサイズは可変となります。

1.  **データセットの準備**: まず、データセット内の全音声クリップの長さ（フレーム数）を事前に計算しておきます。
2.  **長さによるソート**: 学習効率向上のため、インデックスを音声クリップの長さでソートします。これにより、似た長さのサンプルが近くに配置されます。
3.  **カスタムSamplerの実装**: `torch.utils.data.Sampler`を継承したカスタムクラスを作成します。
    - バッチ作成ロジック:
        - ソートされたインデックスリストから順にサンプルを追加します。
        - バッチ内のサンプル数が1になったら、そのサンプルの長さを`max_len`とします。
        - 次のサンプルを追加する際、` (現在のバッチ内のサンプル数 + 1) * max(max_len, 新しいサンプルの長さ)` が閾値（`batch_frames`）を超えないかチェックします。
        - 閾値を超えなければサンプルをバッチに追加し、超える場合は現在のバッチを確定し、新しいバッチを開始します。
4.  **閾値の設定**: `batch_frames` は `平均バッチサイズ × 平均フレーム長` を目安に設定します。例えば、平均フレーム長が3秒（600フレーム）なら、`17 * 600 = 10200` フレームを閾値とします。
5.  **DataLoaderへの適用**: 作成したカスタムSamplerを`DataLoader`の`batch_sampler`引数に渡します。`batch_size`は`1`、`shuffle`は`False`に設定します（シャッフルはSampler側で制御）。

**実装イメージ:**
```python
from torch.utils.data import Sampler
import numpy as np

class DynamicBatchSampler(Sampler):
    def __init__(self, lengths, batch_frames, shuffle=True):
        self.lengths = np.array(lengths)
        self.batch_frames = batch_frames
        self.shuffle = shuffle
        
        # 長さでソートしたインデックス
        self.indices = np.argsort(self.lengths)
        
    def __iter__(self):
        if self.shuffle:
            # 学習の多様性を保つため、ある程度のランダム性を加える
            # e.g., チャンクに分けてチャンク内とチャンク間をシャッフル
            # ここでは簡略化
            np.random.shuffle(self.indices)

        batches = []
        current_batch = []
        max_len_in_batch = 0
        
        for idx in self.indices:
            current_batch.append(idx)
            max_len_in_batch = max(max_len_in_batch, self.lengths[idx])
            
            # 閾値チェック
            if (len(current_batch) * max_len_in_batch) > self.batch_frames:
                # 最後の要素を除いてバッチを確定
                batches.append(current_batch[:-1])
                # 新しいバッチを開始
                current_batch = current_batch[-1:]
                max_len_in_batch = self.lengths[current_batch[0]]

        if current_batch:
            batches.append(current_batch)
            
        if self.shuffle:
            np.random.shuffle(batches)
            
        for batch in batches:
            yield batch
            
    def __len__(self):
        # 正確な長さの計算は複雑なので、概算でも可
        return (len(self.indices) + 16) // 17
```

#### 参考資料
- Hayashi et al. 2020: "ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit"
- [PyTorch公式ドキュメント: Sampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler)

#### 残課題
- シャッフル戦略の最適化が必要です。完全にソートすると学習初期のバイアスが大きくなるため、適度なランダム性（例：バケットごとにシャッフル）を導入することが推奨されます。

---

以上で高優先項目の調査を完了します。これらの実装方針に基づき、Phase 1以降の実装を開始することが可能です。残りの項目については、各Phaseの進行に合わせて調査を進めます。